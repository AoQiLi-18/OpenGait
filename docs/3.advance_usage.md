# Advance Usage
### Cross-Dataset Evalution
> You can achieve cross-dataset evalution by just modifying several arguments in your [data_cfg](../config/baseline.yaml#L1).
>
>  Take [baseline.yaml](../config/baseline.yaml) as an example:
> ```yaml
> data_cfg:
>   dataset_name: CASIA-B
>   dataset_root:  your_path
>   dataset_partition: ./misc/partitions/CASIA-B_include_005.json
>   num_workers: 1
>   remove_no_gallery: false # Remove probe if no gallery for it
>   test_dataset_name: CASIA-B
> ```
> Assume we get the model trained on [CASIA-B](http://www.cbsr.ia.ac.cn/english/Gait%20Databases.asp), and now we want to test it on [OUMVLP](http://www.am.sanken.osaka-u.ac.jp/BiometricDB/GaitMVLP.html).
> 
> Alter the `dataset_root`, `dataset_partition` and `test_dataset_name`, please, just like:
> ```yaml
> data_cfg:
>   dataset_name: CASIA-B
>   dataset_root:  your_OUMVLP_path
>   dataset_partition: ./misc/partitions/OUMVLP.json
>   num_workers: 1
>   remove_no_gallery: false # Remove probe if no gallery for it
>   test_dataset_name: OUMVLP
> ```
---
>
<!-- ### Identification Function
> Sometime, your test dataset may be neither the popular [CASIA-B](http://www.cbsr.ia.ac.cn/english/Gait%20Databases.asp) nor the largest [OUMVLP](http://www.am.sanken.osaka-u.ac.jp/BiometricDB/GaitMVLP.html). Meanwhile, you need to customize a special identification function to fit your dataset. 
> 
> * If your path structure is similar to [CASIA-B](http://www.cbsr.ia.ac.cn/english/Gait%20Databases.asp) (the 3-flod style: `id-type-view`), we recommand you to  -->

### Data Augmentation
> In OpenGait, there is a basic transform class used in almost all the models, that is [BaseSilCuttingTransform](../lib/data/transform.py#L20), which is applied to cut the input silhouettes.
>
> Therefore, by referring to this implementation, you can easily customize the data agumentation in just two steps:
> * *Step1*: Define the transform function/class in [transform.py](../lib/data/transform.py), and make sure it callable. The style of [torchvision.transforms](https://pytorch.org/vision/stable/_modules/torchvision/transforms/transforms.html) is recommanded, and following shows a demo;
>> ```python
>> import torchvision.transforms as T
>> class demo1():
>>     def __init__(self, args):
>>         pass
>>     
>>     def __call__(self, seqs):
>>         '''
>>             seqs: with dimension of [sequence, height, width]
>>         '''
>>         pass
>>         return seqs
>> 
>> class demo2():
>>     def __init__(self, args):
>>         pass
>>     
>>     def __call__(self, seqs):
>>         pass
>>         return seqs
>> 
>>  def TransformDemo(base_args, demo1_args, demo2_args):
>>     transform = T.Compose([
>>         BaseSilCuttingTransform(**base_args), 
>>         demo1(args=demo1_args), 
>>         demo2(args=demo2_args)
>>     ])
>>     return transform
>> ```
> * *Step2*: Reset the [`transform`](../config/baseline.yaml#L100) arguments in your config file:
>> ```yaml
>> transform:
>> - type: TransformDemo
>>     base_args: {'img_w': 64}
>>     demo1_args: false
>>     demo2_args: false
>> ```

### Customize Loss

### Visualization